{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":24286,"databundleVersionId":1878097,"sourceType":"competition"}],"dockerImageVersionId":30086,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk\n# !pip install transformers[torch]\n# !pip install accelerate -U\n\n! pip install timm==0.6.7\n!pip install transformers\n!pip install opencv-python","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYcXESY4Nsas","outputId":"ebfec02a-f948-457b-b900-7901c83da20c","execution":{"iopub.status.busy":"2024-04-05T06:27:10.140038Z","iopub.execute_input":"2024-04-05T06:27:10.140433Z","iopub.status.idle":"2024-04-05T06:27:38.930203Z","shell.execute_reply.started":"2024-04-05T06:27:10.140349Z","shell.execute_reply":"2024-04-05T06:27:38.929353Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.15.0)\nCollecting timm==0.6.7\n  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n\u001b[K     |████████████████████████████████| 509 kB 6.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.6.7) (0.8.1)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.6.7) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.6.7) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.6.7) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.6.7) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.6.7) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.6.7) (7.2.0)\nInstalling collected packages: timm\nSuccessfully installed timm-0.6.7\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.4.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.11.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.56.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (4.5.1.48)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python) (1.19.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#torch\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.optim.lr_scheduler import _LRScheduler","metadata":{"id":"AhOofCfdPUZc","execution":{"iopub.status.busy":"2024-04-05T06:27:38.932663Z","iopub.execute_input":"2024-04-05T06:27:38.933053Z","iopub.status.idle":"2024-04-05T06:27:43.079208Z","shell.execute_reply.started":"2024-04-05T06:27:38.933011Z","shell.execute_reply":"2024-04-05T06:27:43.078338Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"DIM = (512,512)\n\nNUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 5\nSEED = 42\n#LR = 3e-4\n\ndevice = torch.device('cuda')\n\n\n################################################# MODEL ####################################################################\n\nmodel_name = 'efficientnet_b3' #efficientnet_b0-b7\n\n################################################ Metric Loss and its params #######################################################\nloss_module = 'arcface' #'cosface' #'adacos'\ns = 30.0\nm = 0.5\nls_eps = 0.0\neasy_margin = False\n\n\n\nscheduler_params = {\n        \"lr_start\": 1e-5,\n        \"lr_max\": 1e-5 * TRAIN_BATCH_SIZE,\n        \"lr_min\": 1e-6,\n        \"lr_ramp_ep\": 5,\n        \"lr_sus_ep\": 0,\n        \"lr_decay\": 0.8,\n    }\n\n############################################## Model Params ###############################################################\nmodel_params = {\n    'n_classes':11014,\n    'model_name':'efficientnet_b3',\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.0,\n    'loss_module':loss_module,\n    's':30.0,\n    'margin':0.50,\n    'ls_eps':0.0,\n    'theta_zero':0.785,\n    'pretrained':True\n}","metadata":{"id":"BAB2uECkNfoY","execution":{"iopub.status.busy":"2024-04-05T06:27:43.080622Z","iopub.execute_input":"2024-04-05T06:27:43.080901Z","iopub.status.idle":"2024-04-05T06:27:43.090253Z","shell.execute_reply.started":"2024-04-05T06:27:43.080871Z","shell.execute_reply":"2024-04-05T06:27:43.088963Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n  random.seed(seed)\n  os.environ['PYTHONHASHSEED'] = str(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  torch.cuda.manual_seed(seed)\n  torch.backends.cudnn.deterministic = True\n\nseed_torch(SEED)\n","metadata":{"id":"u8J2XPb7NlYH","execution":{"iopub.status.busy":"2024-04-05T06:27:43.091707Z","iopub.execute_input":"2024-04-05T06:27:43.092076Z","iopub.status.idle":"2024-04-05T06:27:43.111863Z","shell.execute_reply.started":"2024-04-05T06:27:43.092040Z","shell.execute_reply":"2024-04-05T06:27:43.111063Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n  def __init__(self):\n      self.reset()\n\n  def reset(self):\n      self.val = 0\n      self.avg = 0\n      self.sum = 0\n      self.count = 0\n\n  def update(self, val, n=1):\n      self.val = val\n      self.sum += val * n\n      self.count += n\n      self.avg = self.sum / self.count","metadata":{"id":"Kgkl3hPsNpNb","execution":{"iopub.status.busy":"2024-04-05T06:27:43.114440Z","iopub.execute_input":"2024-04-05T06:27:43.114723Z","iopub.status.idle":"2024-04-05T06:27:43.120720Z","shell.execute_reply.started":"2024-04-05T06:27:43.114689Z","shell.execute_reply":"2024-04-05T06:27:43.120070Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n  if SCHEDULER =='ReduceLROnPlateau':\n      scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True, eps=eps)\n  elif SCHEDULER =='CosineAnnealingLR':\n      scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=min_lr, last_epoch=-1)\n  elif SCHEDULER =='CosineAnnealingWarmRestarts':\n      scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=1, eta_min=min_lr, last_epoch=-1)\n  return scheduler","metadata":{"id":"zM7o7mnIRJxZ","execution":{"iopub.status.busy":"2024-04-05T06:27:43.123234Z","iopub.execute_input":"2024-04-05T06:27:43.123500Z","iopub.status.idle":"2024-04-05T06:27:43.132454Z","shell.execute_reply.started":"2024-04-05T06:27:43.123474Z","shell.execute_reply":"2024-04-05T06:27:43.131692Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def fetch_loss():\n  loss = nn.CrossEntropyLoss()\n  return loss","metadata":{"id":"3XgpF21zRSkE","execution":{"iopub.status.busy":"2024-04-05T06:27:43.133688Z","iopub.execute_input":"2024-04-05T06:27:43.134068Z","iopub.status.idle":"2024-04-05T06:27:43.146648Z","shell.execute_reply.started":"2024-04-05T06:27:43.134033Z","shell.execute_reply":"2024-04-05T06:27:43.145784Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n  return albumentations.Compose(\n      [\n          albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n          albumentations.HorizontalFlip(p=0.5),\n          albumentations.VerticalFlip(p=0.5),\n          albumentations.Rotate(limit=120, p=0.8),\n          albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n          #albumentations.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n          #albumentations.ShiftScaleRotate(\n            #  shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n          #),\n          albumentations.Normalize(),\n          ToTensorV2(p=1.0),\n      ]\n  )\n\ndef get_valid_transforms():\n  return albumentations.Compose(\n      [\n          albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n          albumentations.Normalize(),\n      ToTensorV2(p=1.0)\n      ]\n  )","metadata":{"id":"MA10JcR6RV-M","execution":{"iopub.status.busy":"2024-04-05T06:27:43.148038Z","iopub.execute_input":"2024-04-05T06:27:43.148397Z","iopub.status.idle":"2024-04-05T06:27:43.157370Z","shell.execute_reply.started":"2024-04-05T06:27:43.148362Z","shell.execute_reply":"2024-04-05T06:27:43.156527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class EfficientDataset(Dataset):\n  def __init__(self, csv, transforms=None):\n\n      self.csv = csv.reset_index()\n      self.augmentations = transforms\n\n  def __len__(self):\n      return self.csv.shape[0]\n\n  def __getitem__(self, index):\n      row = self.csv.iloc[index]\n\n      text = row.title\n\n      image = cv2.imread(row.filepath)\n      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n      if self.augmentations:\n          augmented = self.augmentations(image=image)\n          image = augmented['image']\n\n\n      return image,torch.tensor(row.label_group)","metadata":{"id":"xxu7Ihg6RaNr","execution":{"iopub.status.busy":"2024-04-05T06:27:43.158372Z","iopub.execute_input":"2024-04-05T06:27:43.158674Z","iopub.status.idle":"2024-04-05T06:27:43.169377Z","shell.execute_reply.started":"2024-04-05T06:27:43.158647Z","shell.execute_reply":"2024-04-05T06:27:43.168430Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class EfficientNet(nn.Module):\n\n  def __init__(self,\n                n_classes,\n                model_name='efficientnet_b0',\n                use_fc=False,\n                fc_dim=512,\n                dropout=0.0,\n                loss_module='softmax',\n                s=30.0,\n                margin=0.50,\n                ls_eps=0.0,\n                theta_zero=0.785,\n                pretrained=True):\n      \"\"\"\n      :param n_classes:\n      :param model_name: name of model from pretrainedmodels\n          e.g. resnet50, resnext101_32x4d, pnasnet5large\n      :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n      :param loss_module: One of ('arcface', 'cosface', 'softmax')\n      \"\"\"\n      super(EfficientNet, self).__init__()\n      print('Building Model Backbone for {} model'.format(model_name))\n\n      self.backbone = timm.create_model(model_name, pretrained=pretrained)\n      final_in_features = self.backbone.classifier.in_features\n\n      self.backbone.classifier = nn.Identity()\n      self.backbone.global_pool = nn.Identity()\n\n      self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n      self.use_fc = use_fc\n      if use_fc:\n          self.dropout = nn.Dropout(p=dropout)\n          self.fc = nn.Linear(final_in_features, fc_dim)\n          self.bn = nn.BatchNorm1d(fc_dim)\n          self._init_params()\n          final_in_features = fc_dim\n\n      self.loss_module = loss_module\n#       if loss_module == 'arcface':\n      self.final = ArcMarginProduct(final_in_features, n_classes,\n                                    s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n#       elif loss_module == 'cosface':\n#           self.final = AddMarginProduct(final_in_features, n_classes, s=s, m=margin)\n#       elif loss_module == 'adacos':\n#           self.final = AdaCos(final_in_features, n_classes, m=margin, theta_zero=theta_zero)\n#       else:\n#           self.final = nn.Linear(final_in_features, n_classes)\n\n  def _init_params(self):\n      nn.init.xavier_normal_(self.fc.weight)\n      nn.init.constant_(self.fc.bias, 0)\n      nn.init.constant_(self.bn.weight, 1)\n      nn.init.constant_(self.bn.bias, 0)\n\n  def forward(self, x, label):\n      feature = self.extract_feat(x)\n      if self.loss_module in ('arcface', 'cosface', 'adacos'):\n          logits = self.final(feature, label)\n      else:\n          logits = self.final(feature)\n      return logits\n\n  def extract_feat(self, x):\n      batch_size = x.shape[0]\n      x = self.backbone(x)\n      x = self.pooling(x).view(batch_size, -1)\n\n      if self.use_fc:\n          x = self.dropout(x)\n          x = self.fc(x)\n          x = self.bn(x)\n\n      return x","metadata":{"id":"lzUBTVqkReM6","execution":{"iopub.status.busy":"2024-04-05T06:31:12.962883Z","iopub.execute_input":"2024-04-05T06:31:12.963249Z","iopub.status.idle":"2024-04-05T06:31:12.979315Z","shell.execute_reply.started":"2024-04-05T06:31:12.963217Z","shell.execute_reply":"2024-04-05T06:31:12.978512Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# class AdaCos(nn.Module):\n#   def __init__(self, in_features, out_features, m=0.50, ls_eps=0, theta_zero=math.pi/4):\n#       super(AdaCos, self).__init__()\n#       self.in_features = in_features\n#       self.out_features = out_features\n#       self.theta_zero = theta_zero\n#       self.s = math.log(out_features - 1) / math.cos(theta_zero)\n#       self.m = m\n#       self.ls_eps = ls_eps  # label smoothing\n#       self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n#       nn.init.xavier_uniform_(self.weight)\n\n#   def forward(self, input, label):\n#       # normalize features\n#       x = F.normalize(input)\n#       # normalize weights\n#       W = F.normalize(self.weight)\n#       # dot product\n#       logits = F.linear(x, W)\n#       # add margin\n#       theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n#       target_logits = torch.cos(theta + self.m)\n#       one_hot = torch.zeros_like(logits)\n#       one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n#       if self.ls_eps > 0:\n#           one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n#       output = logits * (1 - one_hot) + target_logits * one_hot\n#       # feature re-scale\n#       with torch.no_grad():\n#           B_avg = torch.where(one_hot < 1, torch.exp(self.s * logits), torch.zeros_like(logits))\n#           B_avg = torch.sum(B_avg) / input.size(0)\n#           theta_med = torch.median(theta)\n#           self.s = torch.log(B_avg) / torch.cos(torch.min(self.theta_zero * torch.ones_like(theta_med), theta_med))\n#       output *= self.s\n\n#       return output","metadata":{"id":"f94jWrYARjSA","execution":{"iopub.status.busy":"2024-04-05T06:31:13.194201Z","iopub.execute_input":"2024-04-05T06:31:13.194510Z","iopub.status.idle":"2024-04-05T06:31:13.206838Z","shell.execute_reply.started":"2024-04-05T06:31:13.194462Z","shell.execute_reply":"2024-04-05T06:31:13.205863Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"id":"flbTfQyZRoBg","execution":{"iopub.status.busy":"2024-04-05T06:31:13.511678Z","iopub.execute_input":"2024-04-05T06:31:13.512030Z","iopub.status.idle":"2024-04-05T06:31:13.524375Z","shell.execute_reply.started":"2024-04-05T06:31:13.511986Z","shell.execute_reply":"2024-04-05T06:31:13.523422Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# class AddMarginProduct(nn.Module):\n#     r\"\"\"Implement of large margin cosine distance: :\n#     Args:\n#         in_features: size of each input sample\n#         out_features: size of each output sample\n#         s: norm of input feature\n#         m: margin\n#         cos(theta) - m\n#     \"\"\"\n\n#     def __init__(self, in_features, out_features, s=30.0, m=0.40):\n#         super(AddMarginProduct, self).__init__()\n#         self.in_features = in_features\n#         self.out_features = out_features\n#         self.s = s\n#         self.m = m\n#         self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n#         nn.init.xavier_uniform_(self.weight)\n\n#     def forward(self, input, label):\n#         # --------------------------- cos(theta) & phi(theta) ---------------------------\n#         cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n#         phi = cosine - self.m\n#         # --------------------------- convert label to one-hot ---------------------------\n#         one_hot = torch.zeros(cosine.size(), device='cuda')\n#         # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n#         one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n#         # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n#         output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n#         output *= self.s\n#         # print(output)\n\n#         return output","metadata":{"id":"j67mStQBRqo0","execution":{"iopub.status.busy":"2024-04-05T06:31:13.767121Z","iopub.execute_input":"2024-04-05T06:31:13.767421Z","iopub.status.idle":"2024-04-05T06:31:13.776112Z","shell.execute_reply.started":"2024-04-05T06:31:13.767391Z","shell.execute_reply":"2024-04-05T06:31:13.775233Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class ShopeeScheduler(_LRScheduler):\n    def __init__(self, optimizer, lr_start=5e-6, lr_max=1e-5,\n                 lr_min=1e-6, lr_ramp_ep=5, lr_sus_ep=0, lr_decay=0.8,\n                 last_epoch=-1):\n        self.lr_start = lr_start\n        self.lr_max = lr_max\n        self.lr_min = lr_min\n        self.lr_ramp_ep = lr_ramp_ep\n        self.lr_sus_ep = lr_sus_ep\n        self.lr_decay = lr_decay\n        super(ShopeeScheduler, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n\n        if self.last_epoch == 0:\n            self.last_epoch += 1\n            return [self.lr_start for _ in self.optimizer.param_groups]\n\n        lr = self._compute_lr_from_epoch()\n        self.last_epoch += 1\n\n        return [lr for _ in self.optimizer.param_groups]\n\n    def _get_closed_form_lr(self):\n        return self.base_lrs\n\n    def _compute_lr_from_epoch(self):\n        if self.last_epoch < self.lr_ramp_ep:\n            lr = ((self.lr_max - self.lr_start) /\n                  self.lr_ramp_ep * self.last_epoch +\n                  self.lr_start)\n\n        elif self.last_epoch < self.lr_ramp_ep + self.lr_sus_ep:\n            lr = self.lr_max\n\n        else:\n            lr = ((self.lr_max - self.lr_min) * self.lr_decay**\n                  (self.last_epoch - self.lr_ramp_ep - self.lr_sus_ep) +\n                  self.lr_min)\n        return lr","metadata":{"id":"W7asdFARRrJc","execution":{"iopub.status.busy":"2024-04-05T06:31:14.007345Z","iopub.execute_input":"2024-04-05T06:31:14.007613Z","iopub.status.idle":"2024-04-05T06:31:14.018101Z","shell.execute_reply.started":"2024-04-05T06:31:14.007588Z","shell.execute_reply":"2024-04-05T06:31:14.017373Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def train_fn(dataloader,model,criterion,optimizer,device,scheduler,epoch):\n    model.train()\n    loss_score = AverageMeter()\n\n    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n    for bi,d in tk0:\n\n        batch_size = d[0].shape[0]\n\n\n        images = d[0]\n        targets = d[1]\n\n        images = images.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(images,targets)\n\n        loss = criterion(output,targets)\n\n        loss.backward()\n        optimizer.step()\n\n        loss_score.update(loss.detach().item(), batch_size)\n        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch,LR=optimizer.param_groups[0]['lr'])\n\n    if scheduler is not None:\n            scheduler.step()\n\n    return loss_score","metadata":{"id":"TWCjaIJxRtwn","execution":{"iopub.status.busy":"2024-04-05T06:31:14.286376Z","iopub.execute_input":"2024-04-05T06:31:14.286714Z","iopub.status.idle":"2024-04-05T06:31:14.295005Z","shell.execute_reply.started":"2024-04-05T06:31:14.286676Z","shell.execute_reply":"2024-04-05T06:31:14.293929Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def eval_fn(data_loader,model,criterion,device):\n\n    loss_score = AverageMeter()\n\n    model.eval()\n    tk0 = tqdm(enumerate(data_loader), total=len(data_loader))\n\n    with torch.no_grad():\n\n        for bi,d in tk0:\n            batch_size = d[0].size()[0]\n\n            image = d[0]\n            targets = d[1]\n\n            image = image.to(device)\n            targets = targets.to(device)\n\n            output = model(image,targets)\n\n            loss = criterion(output,targets)\n\n            loss_score.update(loss.detach().item(), batch_size)\n            tk0.set_postfix(Eval_Loss=loss_score.avg)\n\n    return loss_score","metadata":{"id":"erv2USvsRwHU","execution":{"iopub.status.busy":"2024-04-05T06:31:14.686613Z","iopub.execute_input":"2024-04-05T06:31:14.686947Z","iopub.status.idle":"2024-04-05T06:31:14.694164Z","shell.execute_reply.started":"2024-04-05T06:31:14.686917Z","shell.execute_reply":"2024-04-05T06:31:14.693204Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef create_folds(data, num_splits):\n  # Khởi tạo KFold với số fold là 5\n  kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n\n  # Thêm cột fold vào DataFrame và gán giá trị cho từng fold\n  for fold, (train_index, val_index) in enumerate(kf.split(data), 1):\n    data.loc[val_index, 'fold'] = int(fold)\n  return data","metadata":{"id":"c8XNW7QjUuyE","execution":{"iopub.status.busy":"2024-04-05T06:31:15.158494Z","iopub.execute_input":"2024-04-05T06:31:15.158825Z","iopub.status.idle":"2024-04-05T06:31:15.164366Z","shell.execute_reply.started":"2024-04-05T06:31:15.158794Z","shell.execute_reply":"2024-04-05T06:31:15.163509Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/shopee-product-matching/train.csv')\ndata['filepath'] = data['image'].apply(lambda x: os.path.join('../input/shopee-product-matching', 'train_images', x))\ndata = create_folds(data, num_splits=5)","metadata":{"id":"_kbSuw1MRyYO","execution":{"iopub.status.busy":"2024-04-05T06:31:15.495594Z","iopub.execute_input":"2024-04-05T06:31:15.495927Z","iopub.status.idle":"2024-04-05T06:31:15.712404Z","shell.execute_reply.started":"2024-04-05T06:31:15.495898Z","shell.execute_reply":"2024-04-05T06:31:15.711518Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"WY4p0-pYR7o2","outputId":"a41e9cbd-637e-469e-d0fa-2d86b1fb1b99","execution":{"iopub.status.busy":"2024-04-05T06:31:16.116377Z","iopub.execute_input":"2024-04-05T06:31:16.116700Z","iopub.status.idle":"2024-04-05T06:31:16.129689Z","shell.execute_reply.started":"2024-04-05T06:31:16.116672Z","shell.execute_reply":"2024-04-05T06:31:16.128775Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                            filepath  fold  \n0  ../input/shopee-product-matching/train_images/...   1.0  \n1  ../input/shopee-product-matching/train_images/...   4.0  \n2  ../input/shopee-product-matching/train_images/...   4.0  \n3  ../input/shopee-product-matching/train_images/...   2.0  \n4  ../input/shopee-product-matching/train_images/...   1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>filepath</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"encoder = LabelEncoder()\ndata['label_group'] = encoder.fit_transform(data['label_group'])\ndata = data.head(2000)","metadata":{"id":"dhKr8hXIR9Ke","execution":{"iopub.status.busy":"2024-04-05T06:31:16.590491Z","iopub.execute_input":"2024-04-05T06:31:16.590828Z","iopub.status.idle":"2024-04-05T06:31:16.598869Z","shell.execute_reply.started":"2024-04-05T06:31:16.590797Z","shell.execute_reply":"2024-04-05T06:31:16.597903Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def run():\n\n  train = data[data['fold']!=1].reset_index(drop=True)\n  valid = data[data['fold']==1].reset_index(drop=True)\n  # Defining DataSet\n  train_dataset = EfficientDataset(\n      csv=train,\n      transforms=get_train_transforms(),\n  )\n\n  valid_dataset = EfficientDataset(\n      csv=valid,\n      transforms=get_valid_transforms(),\n  )\n\n  train_loader = torch.utils.data.DataLoader(\n      train_dataset,\n      batch_size=TRAIN_BATCH_SIZE,\n      pin_memory=True,\n      drop_last=True,\n      num_workers=NUM_WORKERS\n  )\n\n  valid_loader = torch.utils.data.DataLoader(\n      valid_dataset,\n      batch_size=VALID_BATCH_SIZE,\n      num_workers=NUM_WORKERS,\n      shuffle=False,\n      pin_memory=True,\n      drop_last=False,\n  )\n\n  # Defining Device\n  if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n  else:\n    device=torch.device('cpu')\n\n  # Defining Model for specific fold\n  model = EfficientNet(**model_params)\n  # model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n  model.to(device)\n\n  #DEfining criterion\n  criterion = fetch_loss()\n  criterion.to(device)\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=scheduler_params['lr_start'])\n\n  #Defining LR SCheduler\n  scheduler = ShopeeScheduler(optimizer,**scheduler_params)\n\n  # TRAIN LOOP\n  best_loss = 10000\n  for epoch in range(EPOCHS):\n      train_loss = train_fn(train_loader, model,criterion, optimizer, device,scheduler=scheduler,epoch=epoch)\n\n      valid_loss = eval_fn(valid_loader, model, criterion,device)\n      print(f'val_loss_{epoch}: {valid_loss.avg}')\n      if valid_loss.avg < best_loss:\n          best_loss = valid_loss.avg\n          torch.save(model.state_dict(),f'model_{model_name}_IMG_SIZE_{DIM[0]}_{loss_module}.bin')\n          print('best model found for epoch {}'.format(epoch))\n      torch.save(model.state_dict(),f'model_{model_name}_IMG_SIZE_{DIM[0]}_EPOCH_{epoch}_{loss_module}.bin')","metadata":{"id":"6QecI95eSAJL","execution":{"iopub.status.busy":"2024-04-05T06:31:17.254243Z","iopub.execute_input":"2024-04-05T06:31:17.254550Z","iopub.status.idle":"2024-04-05T06:31:17.266942Z","shell.execute_reply.started":"2024-04-05T06:31:17.254523Z","shell.execute_reply":"2024-04-05T06:31:17.265972Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"run()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TKDygWqSEy6","outputId":"d8bf197a-ec84-4f35-83eb-40039fc8f934","execution":{"iopub.status.busy":"2024-04-05T06:31:18.479824Z","iopub.execute_input":"2024-04-05T06:31:18.480234Z","iopub.status.idle":"2024-04-05T06:36:38.556254Z","shell.execute_reply.started":"2024-04-05T06:31:18.480198Z","shell.execute_reply":"2024-04-05T06:36:38.555366Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Building Model Backbone for efficientnet_b3 model\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n100%|██████████| 101/101 [00:57<00:00,  1.74it/s, Epoch=0, LR=1e-5, Train_Loss=24] \n100%|██████████| 24/24 [00:05<00:00,  4.53it/s, Eval_Loss=23.8]\n","output_type":"stream"},{"name":"stdout","text":"val_loss_0: 23.84964332879368\nbest model found for epoch 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 101/101 [00:56<00:00,  1.78it/s, Epoch=1, LR=7e-5, Train_Loss=22] \n100%|██████████| 24/24 [00:04<00:00,  4.94it/s, Eval_Loss=23.8]\n","output_type":"stream"},{"name":"stdout","text":"val_loss_1: 23.847656967123246\nbest model found for epoch 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 101/101 [00:56<00:00,  1.79it/s, Epoch=2, LR=0.00013, Train_Loss=20.1]\n100%|██████████| 24/24 [00:04<00:00,  4.86it/s, Eval_Loss=23.5]\n","output_type":"stream"},{"name":"stdout","text":"val_loss_2: 23.457274245219814\nbest model found for epoch 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 101/101 [00:56<00:00,  1.79it/s, Epoch=3, LR=0.000128, Train_Loss=18] \n100%|██████████| 24/24 [00:05<00:00,  4.79it/s, Eval_Loss=23.1]\n","output_type":"stream"},{"name":"stdout","text":"val_loss_3: 23.133458000561586\nbest model found for epoch 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 101/101 [00:56<00:00,  1.79it/s, Epoch=4, LR=8.24e-5, Train_Loss=16] \n100%|██████████| 24/24 [00:05<00:00,  4.63it/s, Eval_Loss=22.9]\n","output_type":"stream"},{"name":"stdout","text":"val_loss_4: 22.92686398160053\nbest model found for epoch 4\n","output_type":"stream"}]}]}